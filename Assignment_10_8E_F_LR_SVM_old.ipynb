{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "8E_F_LR_SVM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HExLQrE4ZxR"
      },
      "source": [
        "<h1><font color='blue'> 8E and 8F: Finding the Probability P(Y==1|X)</font></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LuKrFzC4ZxV"
      },
      "source": [
        "<h2><font color='Geen'> 8E: Implementing Decision Function of SVM RBF Kernel</font></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wES-wWN4ZxX"
      },
      "source": [
        "<font face=' Comic Sans MS' size=3>After we train a kernel SVM model, we will be getting support vectors and their corresponsing coefficients $\\alpha_{i}$\n",
        "\n",
        "Check the documentation for better understanding of these attributes: \n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "<img src='https://i.imgur.com/K11msU4.png' width=500>\n",
        "\n",
        "As a part of this assignment you will be implementing the ```decision_function()``` of kernel SVM, here decision_function() means based on the value return by ```decision_function()``` model will classify the data point either as positive or negative\n",
        "\n",
        "Ex 1: In logistic regression After traning the models with the optimal weights $w$ we get, we will find the value $\\frac{1}{1+\\exp(-(wx+b))}$, if this value comes out to be < 0.5 we will mark it as negative class, else its positive class\n",
        "\n",
        "Ex 2: In Linear SVM After traning the models with the optimal weights $w$ we get, we will find the value of $sign(wx+b)$, if this value comes out to be -ve we will mark it as negative class, else its positive class.\n",
        "\n",
        "Similarly in Kernel SVM After traning the models with the coefficients $\\alpha_{i}$ we get, we will find the value of \n",
        "$sign(\\sum_{i=1}^{n}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here $K(x_{i},x_{q})$ is the RBF kernel. If this value comes out to be -ve we will mark $x_{q}$ as negative class, else its positive class.\n",
        "\n",
        "RBF kernel is defined as: $K(x_{i},x_{q})$ = $exp(-\\gamma ||x_{i} - x_{q}||^2)$\n",
        "\n",
        "For better understanding check this link: https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z830CfMk4Zxa"
      },
      "source": [
        "## Task E"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuBxHiCQ4Zxc"
      },
      "source": [
        "> 1. Split the data into $X_{train}$(60), $X_{cv}$(20), $X_{test}$(20)\n",
        "\n",
        "> 2. Train $SVC(gamma=0.001, C=100.)$ on the ($X_{train}$, $y_{train}$)\n",
        "\n",
        "> 3. Get the decision boundry values $f_{cv}$ on the $X_{cv}$ data  i.e. ` `$f_{cv}$ ```= decision_function(```$X_{cv}$```)```  <font color='red'>you need to implement this decision_function()</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCgMNEvI4Zxf"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANUNIqCe4Zxn"
      },
      "source": [
        "X, y = make_classification(n_samples=5000, n_features=5, n_redundant=2,\n",
        "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvV-tNpHOrD7",
        "outputId": "d709c8a3-65ed-45ea-a81d-9214293a00c2"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "clf = SVC(gamma=0.001,C=100)\n",
        "clf.fit(X, y) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHie1zqH4Zxt"
      },
      "source": [
        "### Pseudo code\n",
        "\n",
        "clf = SVC(gamma=0.001, C=100.)<br>\n",
        "clf.fit(Xtrain, ytrain)\n",
        "\n",
        "<font color='green'>def</font> <font color='blue'>decision_function</font>(Xcv, ...): #use appropriate parameters <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='green'>for</font> a data point $x_q$ <font color='green'>in</font> Xcv: <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='grey'>#write code to implement $(\\sum_{i=1}^{\\text{all the support vectors}}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here the values $y_i$, $\\alpha_{i}$, and $intercept$ can be obtained from the trained model</font><br>\n",
        "   <font color='green'>return</font> <font color='grey'><i># the decision_function output for all the data points in the Xcv</i></font>\n",
        "    \n",
        "fcv = decision_function(Xcv, ...)  <i># based on your requirement you can pass any other parameters </i>\n",
        "\n",
        "<b>Note</b>: Make sure the values you get as fcv, should be equal to outputs of clf.decision_function(Xcv)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h43kDT3M41u5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "358cbe1f-7eaf-4d79-f089-5c1c120ed380"
      },
      "source": [
        "clf.decision_function(X)\n",
        "clf.predict(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, ..., 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBsdjlOcQjKb"
      },
      "source": [
        "# I've only implemented the linear and rbf kernels\n",
        "def kernel(params, sv, X):\n",
        "    import math\n",
        "    if params['kernel'] == 'linear':\n",
        "        return [np.dot(vi, X) for vi in sv]\n",
        "    elif params['kernel'] == 'rbf':\n",
        "        return [np.exp(-params['gamma'] * np.dot(X-vi,X-vi)) for vi in sv]\n",
        "\n",
        "# This replicates clf.decision_function(X)\n",
        "def decision_function(params, sv, nv, a, b, X):\n",
        "    # calculate the kernels\n",
        "    k = kernel(params, sv, X)\n",
        "    \n",
        "\n",
        "    # define the start and end index for support vectors for each class\n",
        "    start = [sum(nv[:i]) for i in range(len(nv))]\n",
        "    end = [start[i] + nv[i] for i in range(len(nv))]\n",
        "\n",
        "    # calculate: sum(a_p * k(x_p, x)) between every 2 classes\n",
        "    c = [ sum(a[ i ][p] * k[p] for p in range(start[j], end[j])) +\n",
        "          sum(a[j-1][p] * k[p] for p in range(start[i], end[i]))\n",
        "                for i in range(len(nv)) for j in range(i+1,len(nv))]\n",
        "\n",
        "    # add the intercept\n",
        "    return [sum(x) for x in zip(c, b)]\n",
        "\n",
        "# This replicates clf.predict(X)\n",
        "def predict(params, sv, nv, a, b, cs, X):\n",
        "    ''' params = model parameters\n",
        "        sv = support vectors\n",
        "        nv = # of support vectors per class\n",
        "        a  = dual coefficients\n",
        "        b  = intercepts \n",
        "        cs = list of class names\n",
        "        X  = feature to predict       \n",
        "    '''\n",
        "    decision = decision_function(params, sv, nv, a, b, X)\n",
        "    votes = [(i if decision[p] > 0 else j) for p,(i,j) in enumerate((i,j) \n",
        "                                           for i in range(len(cs))\n",
        "                                           for j in range(i+1,len(cs)))]\n",
        "\n",
        "    return cs[max(set(votes), key=votes.count)]"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "wt8JZpHyQv-S",
        "outputId": "4755d9e3-df28-457a-ba10-b5eada1150de"
      },
      "source": [
        "# Create model\n",
        "from sklearn import svm\n",
        "clf = svm.SVC(gamma=0.001,C=100.)\n",
        "\n",
        "# Fit model using features, X, and labels, Y.\n",
        "clf.fit(X, y)\n",
        "# Get parameters from model\n",
        "params = clf.get_params()\n",
        "\n",
        "sv = clf.support_vectors_\n",
        "nv = clf.n_support_\n",
        "a  = clf.dual_coef_\n",
        "b  = clf._intercept_\n",
        "cs = clf.classes_\n",
        "# Use the functions to predict\n",
        "print(predict(params,sv, nv, a, b, cs, X))\n",
        "\n",
        "# Compare with the builtin predict\n",
        "print(clf.predict(X))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-8c8e41d81c9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Use the functions to predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Compare with the builtin predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-944afc86342a>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(params, sv, nv, a, b, cs, X)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mX\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0mto\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     '''\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mdecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     votes = [(i if decision[p] > 0 else j) for p,(i,j) in enumerate((i,j) \n\u001b[1;32m     39\u001b[0m                                            \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-944afc86342a>\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(params, sv, nv, a, b, X)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# calculate the kernels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-944afc86342a>\u001b[0m in \u001b[0;36mkernel\u001b[0;34m(params, sv, X)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'rbf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gamma'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mvi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mvi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# This replicates clf.decision_function(X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-944afc86342a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'rbf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gamma'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mvi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mvi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# This replicates clf.decision_function(X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (5000,5) and (5000,5) not aligned: 5 (dim 1) != 5000 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0bKCboN4Zxu"
      },
      "source": [
        "<h2><font color='Geen'> 8F: Implementing Platt Scaling to find P(Y==1|X)</font></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMn7OEN94Zxw"
      },
      "source": [
        "Check this <a href='https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a'>PDF</a>\n",
        "<img src='https://i.imgur.com/CAMnVnh.png'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0n5EFkx4Zxz"
      },
      "source": [
        "## TASK F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0HOqVJq4Zx1"
      },
      "source": [
        "\n",
        "> 4. Apply SGD algorithm with ($f_{cv}$, $y_{cv}$) and find the weight $W$ intercept $b$ ```Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)``` \n",
        "\n",
        "> Note1: Don't forget to change the values of $y_{cv}$ as mentioned in the above image. you will calculate y+, y- based on data points in train data\n",
        "\n",
        "> Note2: the Sklearn's SGD algorithm doesn't support the real valued outputs, you need to use the code that was done in the `'Logistic Regression with SGD and L2'` Assignment after modifying loss function, and use same parameters that used in that assignment.\n",
        "<img src='https://i.imgur.com/zKYE9Oc.png'>\n",
        "if Y[i] is 1, it will be replaced with y+ value else it will replaced with y- value\n",
        "\n",
        "> 5. For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTY7z2bd4Zx2"
      },
      "source": [
        "__Note: in the above algorithm, the steps 2, 4 might need hyper parameter tuning, To reduce the complexity of the assignment we are excluding the hyerparameter tuning part, but intrested students can try that__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM3odN1Z4Zx3"
      },
      "source": [
        "\n",
        "If any one wants to try other calibration algorithm istonic regression also please check these tutorials\n",
        "\n",
        "1. http://fa.bianp.net/blog/tag/scikit-learn.html#fn:1\n",
        "\n",
        "2. https://drive.google.com/open?id=1MzmA7QaP58RDzocB0RBmRiWfl7Co_VJ7\n",
        "\n",
        "3. https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a\n",
        "\n",
        "4. https://stat.fandom.com/wiki/Isotonic_regression#Pool_Adjacent_Violators_Algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOcjsr0YuIxb"
      },
      "source": [
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=15)\n",
        "# Standardizing the \n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHRTN1Umw82w",
        "outputId": "ac636a29-06a5-4674-872c-75b9116cfa89"
      },
      "source": [
        "clf=linear_model.SGDClassifier(eta0=0.0001,alpha=0.0001,loss='log',random_state=15,penalty='l2')\n",
        "clf\n",
        "clf.fit(X=X_train,y=y_train)\n",
        "clf.coef_,clf.coef_.shape,clf.intercept_  #return weights,shape of weights,itercept\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 0.10026476,  0.53374949, -1.19908055, -1.03826544,  1.97087882]]),\n",
              " (1, 5),\n",
              " array([-2.63705555]))"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    }
  ]
}